{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "# Packages for data preparation\n",
    "# from sklearn.model_selection import train_test_split # Dont need because data folder has already spilt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#For Training Multinomial Naive Bayess\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explains embeddings very well: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "GloVe embeddings: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for Dani\n",
    "\n",
    "* Glove on s3 ---> can just import Glove, tbh if I need help with memory to save glove model \n",
    "    * I don't need to contunitally import Glove, what if I just import it once theb create and cache my embeedding dictionary\n",
    "* What should I use for a temporary production database? \n",
    "* Do I even need a production database, what if I just overwrite the csv file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sa-emotions/train_data.csv\")\n",
    "\n",
    "X_train = data[\"content\"]\n",
    "# X_train\n",
    "data.head()\n",
    "\n",
    "X_train = X_train[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import nltk\n",
    "import inflect\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "'''\n",
    "By default, all punctuation is removed, turning the texts into space-separated sequences of words\n",
    "'''\n",
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "\n",
    "# def remove_mentions(words):\n",
    "#     return re.sub(r'@[A-Za-z0-9]+','', words)\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "#     words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "X_train = [normalize(tweet) for tweet in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@',\n",
       "  'f',\n",
       "  'f',\n",
       "  'n',\n",
       "  'l',\n",
       "  'u',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'k',\n",
       "  'n',\n",
       "  'w',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'w',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  'n',\n",
       "  'n',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'b',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'b',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'r',\n",
       "  'l',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  'e',\n",
       "  'k',\n",
       "  'n',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'r',\n",
       "  ' ',\n",
       "  '=',\n",
       "  '['],\n",
       " ['l',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  'c',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'g',\n",
       "  'h',\n",
       "  'h',\n",
       "  'h',\n",
       "  'h',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'w',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'l',\n",
       "  'l',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.'],\n",
       " ['f',\n",
       "  'u',\n",
       "  'n',\n",
       "  'e',\n",
       "  'r',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'g',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.'],\n",
       " ['w',\n",
       "  'n',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  '!'],\n",
       " ['@',\n",
       "  'n',\n",
       "  'n',\n",
       "  'c',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'n',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'u',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'k',\n",
       "  'e',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'l',\n",
       "  'l',\n",
       "  '.'],\n",
       " ['r',\n",
       "  'e',\n",
       "  '-',\n",
       "  'p',\n",
       "  'n',\n",
       "  'g',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  '@',\n",
       "  'g',\n",
       "  'h',\n",
       "  'r',\n",
       "  'h',\n",
       "  'one',\n",
       "  'four',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'n',\n",
       "  \"'\",\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'g',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'r',\n",
       "  '?',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'c',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'f',\n",
       "  ' ',\n",
       "  'n',\n",
       "  \"'\",\n",
       "  ' ',\n",
       "  'l',\n",
       "  'k',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n'],\n",
       " [' ',\n",
       "  'h',\n",
       "  'u',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  'e',\n",
       "  'p',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'n',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'n',\n",
       "  'k',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'n',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  \"'\",\n",
       "  ' ',\n",
       "  'r',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'w',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ',',\n",
       "  ' ',\n",
       "  '&',\n",
       "  'p',\n",
       "  ';',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'two',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'n',\n",
       "  'l',\n",
       "  'u',\n",
       "  '!'],\n",
       " ['h',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'p',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'w',\n",
       "  'w',\n",
       "  'w',\n",
       "  '.',\n",
       "  'j',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  '.',\n",
       "  'c',\n",
       "  '/',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'n'],\n",
       " ['@',\n",
       "  'c',\n",
       "  'h',\n",
       "  'r',\n",
       "  'v',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'h',\n",
       "  'r',\n",
       "  'l',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'v',\n",
       "  'e',\n",
       "  '.',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'u'],\n",
       " ['@',\n",
       "  'k',\n",
       "  'e',\n",
       "  'l',\n",
       "  'c',\n",
       "  'u',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  \"'\",\n",
       "  ' ',\n",
       "  'r',\n",
       "  'r',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  ' ',\n",
       "  \"'\",\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  '?'],\n",
       " ['c', 'n', ' ', 'f', 'l', 'l', ' ', 'l', 'e', 'e', 'p'],\n",
       " ['c',\n",
       "  'h',\n",
       "  'k',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  'r'],\n",
       " ['u',\n",
       "  'g',\n",
       "  'h',\n",
       "  '!',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'v',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'p',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'e',\n",
       "  'x',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'u',\n",
       "  'e',\n",
       "  '!'],\n",
       " ['@',\n",
       "  'b',\n",
       "  'r',\n",
       "  'j',\n",
       "  'e',\n",
       "  'n',\n",
       "  'n',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'f',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'n',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'e',\n",
       "  'l',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'r',\n",
       "  'u',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  'c',\n",
       "  'u',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'e',\n",
       "  'e',\n",
       "  'k',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'e',\n",
       "  'e',\n",
       "  'k',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'j',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'l',\n",
       "  'n',\n",
       "  'e',\n",
       "  'l',\n",
       "  'l'],\n",
       " ['g', ' ', 'h', 'e', ' ', 'n', 'e', 'w'],\n",
       " ['h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'r',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'l',\n",
       "  'e',\n",
       "  'c',\n",
       "  'r',\n",
       "  'c',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'n',\n",
       "  'e'],\n",
       " ['@', 'n', 'n', 'r', 'e', 'k', 'e', 'r', 'r', ' ', 'g', 'r', 'e', 'e'],\n",
       " [' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  'e',\n",
       "  'p',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  \"'\",\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  '.',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'n',\n",
       "  '.'],\n",
       " ['@',\n",
       "  'p',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  'z',\n",
       "  'h',\n",
       "  'l',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'e',\n",
       "  'e',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'r',\n",
       "  'e',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'b',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'v',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'e',\n",
       "  'k',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'j',\n",
       "  'u',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'k',\n",
       "  'n',\n",
       "  'w'],\n",
       " ['h',\n",
       "  'w',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'n',\n",
       "  'v',\n",
       "  'n',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'h',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'v',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'w',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'u',\n",
       "  '?',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'n',\n",
       "  'l',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'v',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'f',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'n',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'n',\n",
       "  'k',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'j',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer will convert the tweets to sequences so that they can be passed through embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(train['tweet'])\n",
    "test_tokenized = tk.texts_to_sequences(test['tweet'])\n",
    "\n",
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "**texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texs(all_tweets)\n",
    "train_tokenized = tk.texts_to_sequences(X_train)\n",
    "\n",
    "max_length = 70\n",
    "\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Glove embeddings into dictionary [MEMORY ISSUE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/kaggle/input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Label encoding outcome classes\n",
    "\n",
    "labels = to_categorical(np.asarray(train['target']))\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
    "\n",
    "We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Notes\n",
    "* Need to remove mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Medium Research Notes:\n",
    "\n",
    "If we need to create a embedding mapping for words with training, how does this affect how we embed the test data (streamed tweets)? Maybe just use Glove.\n",
    "\n",
    "Shall we use Embedding layer as our input layer?\n",
    "\n",
    "Preprocessing\n",
    "KD article\n",
    "Padding\n",
    "Remove stop words\n",
    "\n",
    "\n",
    "CNN training is 1.5 times faster than training LSTMs\n",
    "\n",
    "Need to determine the unique vocabulary of training in order to determine embedding vector length, then I think you pad off of that? Not sure about the padding preprocessing step.\n",
    "\n",
    "Pathtoword embeddings is the path where we’ve  downloaded word embeddings via glove or number batch\n",
    "\n",
    "Is it cheating if I understand all the processing of what this article is doing, and add my own preprocessing, and maybe additional steps? \n",
    "https://medium.com/@panghalarsh/sentiment-analysis-in-python-using-keras-glove-twitter-word-embeddings-and-deep-rnn-on-a-combined-580646cb900a\n",
    "\n",
    "Word Embeddings is better than count vectorizers that one hot encodes sentences because the bagofwords model will not scale well to large datasets due to create sparse vectors in high dimensionality. Also we lose semantics through BagofWords approach. Word embeddings are dense vectors with much lower dimensionality. Secondly, the semantic relationships between words are reflected in the distance and direction of the vectors.\n",
    "\n",
    "Should read into each Keras library being utilized and understanding \n",
    "\n",
    "\n",
    "Need to encode output classes (emotions) w/ LabelEncoders\n",
    "\n",
    "Why do we need to Flatten after we do embedding? Like what does that do?\n",
    "\n",
    "In the Embedding layer (which is layer 0 here) we set the weights for the words to those found in the GloVe word embeddings. By setting trainable to False we make sure that the GloVe word embeddings cannot be changed.\n",
    "\n",
    "The best result is achieved with 100-dimensional word embeddings that are trained on the available data. By doing this, we do not take into account the relationships between the words in the tweet. This can be achieved with a recurrent neural network or a 1D convolutional network. But that’s something for a future post.\n",
    "— Bert Carremans \n",
    "\n",
    "\n",
    "Do we need padding when utilizing word embeddings? I saw some Word2Vec tutorials using padding one hot encoded sequences, but my understanding was that we don’t need it for embeddings.\n",
    "https://medium.com/datadriveninvestor/sentiment-analysis-using-embeddings-f3dd99aeaade\n",
    "\n",
    "\n",
    "Why train my own word embeddings at all? Why not always use Glove? It seems to be a better trained for my model’s embedding layer anyways.\n",
    "\n",
    "\n",
    "There are so many state of the art embedding techniques like SkipGrams, FastText, and ELMo. I should investigate these for their result to text summarizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 1 2 3 6]]\n",
      "[[[ 0.01658997 -0.03503971 -0.03524492]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [ 0.02627129 -0.02832104  0.01373578]]\n",
      "\n",
      " [[-0.03747336 -0.02579566  0.03765706]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [-0.04258297  0.01286627  0.01142017]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "model = Sequential()\n",
    "model.add(Embedding(7, 3, input_length=5))\n",
    "# model.add(Flatten())\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.array([[0, 1, 2, 3, 4], [5, 1, 2, 3, 6]])\n",
    "print(input_array)\n",
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on BEW portion\n",
    "* No need "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
