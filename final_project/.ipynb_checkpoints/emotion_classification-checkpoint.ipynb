{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "# Packages for data preparation\n",
    "# from sklearn.model_selection import train_test_split # Dont need because data folder has already spilt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#For Training Multinomial Naive Bayess\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explains embeddings very well: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "GloVe embeddings: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Notes\n",
    "\n",
    "* After I have a working model, I can concatenate training and test sets because my testing will be with streamed tweets! \n",
    "* There is something wrong with the way that I'm preprocessing tweets, or maybe the word embedding files that I'm using does not contain what I expect\n",
    "* Naive Bayes VS Glove Embeddings w/ LSTM VS Glove Embeddings w/ stacked LSTMs VS Embeddings + CNN VS ELMO\n",
    "* When we incorporate backend use the following article for preprocessing:\n",
    "\n",
    "https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf?gi=a2d3e63ba57a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data = pd.read_csv(\"sa-emotions/train_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "\n",
    "data[\"sentiment\"].unique()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data = data[0:50]\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re, string, unicodedata\n",
    "# import re, string, unicodedata\n",
    "# import contractions\n",
    "# import nltk\n",
    "# import inflect\n",
    "\n",
    "# '''\n",
    "# TODO:\n",
    "\n",
    "# Look at Emlo article for preprocessing w/ lambda\n",
    "# Do lemmantizing, not stemming?? Why not use all normalizing techniques?\n",
    "# '''\n",
    "\n",
    "# def remove_non_ascii(words):\n",
    "#     \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "# #     new_words = []\n",
    "#     new_text = ''\n",
    "#     for word in words:\n",
    "\n",
    "#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "# #         new_words.append(new_word)\n",
    "#         new_text += new_word\n",
    "#     return new_text\n",
    "\n",
    "# def to_lowercase(words):\n",
    "#     \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "# #     new_words = []\n",
    "#     new_text = ''\n",
    "#     for word in words:\n",
    "#         new_word = word.lower()\n",
    "# #         new_words.append(new_word)\n",
    "#         new_text += new_word\n",
    "    \n",
    "#     return new_text\n",
    "\n",
    "\n",
    "# # By default, all punctuation is removed, turning the texts into space-separated sequences of words\n",
    "\n",
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "# #     new_words = []\n",
    "#     new_text = ''\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_text += new_word\n",
    "#     return new_text\n",
    "\n",
    "# def replace_numbers(words):\n",
    "#     \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "#     p = inflect.engine()\n",
    "# #     new_words = []\n",
    "#     new_text = ''\n",
    "#     for word in words:\n",
    "#         if word.isdigit():\n",
    "#             new_word = p.number_to_words(word)\n",
    "#             new_text += new_word\n",
    "#         else:\n",
    "#             new_text += word\n",
    "            \n",
    "#     return new_text\n",
    "\n",
    "# def remove_stopwords(words):\n",
    "#     \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "# #     new_words = []\n",
    "#     new_text = ''\n",
    "#     for word in words:\n",
    "#         if word not in stopwords.words('english'):\n",
    "#             new_text += word\n",
    "    \n",
    "#     return new_text\n",
    "\n",
    "\n",
    "# # TODO: replace with lemmanization\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     new_text = ''\n",
    "# #     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         new_text += stem\n",
    "#     return stems\n",
    "\n",
    "\n",
    "# def remove_mentions(words):\n",
    "#     ''' Remove @ mentions'''\n",
    "#     new_text = ''\n",
    "    \n",
    "#     for word in words:\n",
    "#         new_text += re.sub(r'@[A-Za-z0-9]+','', words)\n",
    "        \n",
    "#     print(new_text)\n",
    "#     return new_text\n",
    "\n",
    "\n",
    "# def normalize(words):\n",
    "    \n",
    "#     print(words.split(\" \"))\n",
    "#     for words in words.split(' '):\n",
    "#         words = remove_non_ascii(words)\n",
    "#         words = to_lowercase(words)\n",
    "#         words = remove_punctuation(words)\n",
    "#         words = replace_numbers(words)\n",
    "#         words = remove_stopwords(words)\n",
    "# #     print(words)\n",
    "    \n",
    "# #     return words\n",
    "#     return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "# # data[\"content\"].apply(normalize(data[\"content\"][0]))\n",
    "# # data[\"content\"].apply(lambda x : normalize(x))\n",
    "# # X_train = [normalize(tweet) for tweet in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "words = normalize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@tiffanylue', 'i', 'know', '', 'i', 'was', 'listenin', 'to', 'bad', 'habit', 'earlier', 'and', 'i', 'started', 'freakin', 'at', 'his', 'part', '=[']\n",
      "['Layin', 'n', 'bed', 'with', 'a', 'headache', '', 'ughhhh...waitin', 'on', 'your', 'call...']\n",
      "['Funeral', 'ceremony...gloomy', 'friday...']\n",
      "['wants', 'to', 'hang', 'out', 'with', 'friends', 'SOON!']\n",
      "['@dannycastillo', 'We', 'want', 'to', 'trade', 'with', 'someone', 'who', 'has', 'Houston', 'tickets,', 'but', 'no', 'one', 'will.']\n",
      "['Re-pinging', '@ghostridah14:', 'why', \"didn't\", 'you', 'go', 'to', 'prom?', 'BC', 'my', 'bf', \"didn't\", 'like', 'my', 'friends']\n",
      "['I', 'should', 'be', 'sleep,', 'but', 'im', 'not!', 'thinking', 'about', 'an', 'old', 'friend', 'who', 'I', 'want.', 'but', \"he's\", 'married', 'now.', 'damn,', '&amp;', 'he', 'wants', 'me', '2!', 'scandalous!']\n",
      "['Hmmm.', 'http://www.djhero.com/', 'is', 'down']\n",
      "['@charviray', 'Charlene', 'my', 'love.', 'I', 'miss', 'you']\n",
      "['@kelcouch', \"I'm\", 'sorry', '', 'at', 'least', \"it's\", 'Friday?']\n",
      "['cant', 'fall', 'asleep']\n",
      "['Choked', 'on', 'her', 'retainers']\n",
      "['Ugh!', 'I', 'have', 'to', 'beat', 'this', 'stupid', 'song', 'to', 'get', 'to', 'the', 'next', '', 'rude!']\n",
      "['@BrodyJenner', 'if', 'u', 'watch', 'the', 'hills', 'in', 'london', 'u', 'will', 'realise', 'what', 'tourture', 'it', 'is', 'because', 'were', 'weeks', 'and', 'weeks', 'late', '', 'i', 'just', 'watch', 'itonlinelol']\n",
      "['Got', 'the', 'news']\n",
      "['The', 'storm', 'is', 'here', 'and', 'the', 'electricity', 'is', 'gone']\n",
      "['@annarosekerr', 'agreed']\n",
      "['So', 'sleepy', 'again', 'and', \"it's\", 'not', 'even', 'that', 'late.', 'I', 'fail', 'once', 'again.']\n",
      "['@PerezHilton', 'lady', 'gaga', 'tweeted', 'about', 'not', 'being', 'impressed', 'by', 'her', 'video', 'leaking', 'just', 'so', 'you', 'know']\n",
      "['How', 'are', 'YOU', 'convinced', 'that', 'I', 'have', 'always', 'wanted', 'you?', 'What', 'signals', 'did', 'I', 'give', 'off...damn', 'I', 'think', 'I', 'just', 'lost', 'another', 'friend']\n",
      "['@raaaaaaek', 'oh', 'too', 'bad!', 'I', 'hope', 'it', 'gets', 'better.', \"I've\", 'been', 'having', 'sleep', 'issues', 'lately', 'too']\n",
      "['Wondering', 'why', \"I'm\", 'awake', 'at', '7am,writing', 'a', 'new', 'song,plotting', 'my', 'evil', 'secret', 'plots', 'muahahaha...oh', 'damn', 'it,not', 'secret', 'anymore']\n",
      "['No', 'Topic', 'Maps', 'talks', 'at', 'the', 'Balisage', 'Markup', 'Conference', '2009', '', '', 'Program', 'online', 'at', 'http://tr.im/mL6Z', '(via', '@bobdc)', '#topicmaps']\n",
      "['I', 'ate', 'Something', 'I', \"don't\", 'know', 'what', 'it', 'is...', 'Why', 'do', 'I', 'keep', 'Telling', 'things', 'about', 'food']\n",
      "['so', 'tired', 'and', 'i', 'think', \"i'm\", 'definitely', 'going', 'to', 'get', 'an', 'ear', 'infection.', '', 'going', 'to', 'bed', '&quot;early&quot;', 'for', 'once.']\n",
      "['On', 'my', 'way', 'home', 'n', 'having', '2', 'deal', 'w', 'underage', 'girls', 'drinking', 'gin', 'on', 'da', 'bus', 'while', 'talking', 'bout', 'keggers......damn', 'i', 'feel', 'old']\n",
      "['@IsaacMascote', '', \"i'm\", 'sorry', 'people', 'are', 'so', 'rude', 'to', 'you,', 'isaac,', 'they', 'should', 'get', 'some', 'manners', 'and', 'know', 'better', 'than', 'to', 'be', 'so', 'lewd!']\n",
      "['Damm', 'servers', 'still', 'down', '', 'i', 'need', 'to', 'hit', '80', 'before', 'all', 'the', 'koxpers', 'pass', 'me']\n",
      "['Fudge....', 'Just', \"BS'd\", 'that', 'whole', 'paper....', 'So', 'tired....', 'Ugh', 'I', 'hate', 'school.....', '', 'time', 'to', 'sleep!!!!!!!!!!!']\n",
      "['I', 'HATE', 'CANCER.', 'I', 'HATE', 'IT', 'I', 'HATE', 'IT', 'I', 'HATE', 'IT.']\n",
      "['It', 'is', 'so', 'annoying', 'when', 'she', 'starts', 'typing', 'on', 'her', 'computer', 'in', 'the', 'middle', 'of', 'the', 'night!']\n",
      "['@cynthia_123', 'i', 'cant', 'sleep']\n",
      "['I', 'missed', 'the', 'bl***y', 'bus!!!!!!!!']\n",
      "['feels', 'strong', 'contractions', 'but', 'wants', 'to', 'go', 'out.', '', 'http://plurk.com/p/wxidk']\n",
      "['SoCal!', '', 'stoked.', 'or', 'maybe', 'not..', 'tomorrow']\n",
      "['Screw', 'you', '@davidbrussee!', 'I', 'only', 'have', '3', 'weeks...']\n",
      "['@ether_radio', 'yeah', ':S', 'i', 'feel', 'all', 'funny', 'cause', 'i', \"haven't\", 'slept', 'enough', '', 'i', 'woke', 'my', 'mum', 'up', 'cause', 'i', 'was', 'singing', \"she's\", 'not', 'impressed', ':S', 'you?']\n",
      "['I', 'need', 'skott', 'right', 'now']\n",
      "['has', 'work', 'this', 'afternoon']\n",
      "['@GABBYiSACTiVE', 'Aw', 'you', 'would', 'not', 'unfollow', 'me', 'would', 'you?', 'Then', 'I', 'would', 'cry']\n",
      "['mmm', 'much', 'better', 'day...', 'so', 'far!', \"it's\", 'still', 'quite', 'early.', 'last', 'day', 'of', '#uds']\n",
      "['@DavidArchie', '&lt;3', 'your', 'gonna', 'be', 'the', 'first', '', 'twitter', ';)', 'cause', 'your', 'amazing', 'lol.', 'come', 'to', 'canada', '', 'would', 'do', 'anything', 'to', 'see', 'you', 'perform']\n",
      "['just', 'picked', 'up', 'her', 'Blackberry', 'from', 'the', 'middle', 'of', 'the', 'street!', 'Both', 'she', 'and', 'it', 'are', 'crushed!']\n",
      "['Why', 'do', 'I', 'have', 'the', 'feeling', 'I', 'should', 'be', 'packing', 'and', 'hitting', 'for', 'SFO', 'around', 'this', 'time', 'of', 'the', 'year?', 'I', 'think', \"I'm\", 'missing', 'something...']\n",
      "['@creyes', 'middle', 'school', 'and', 'elem.', 'High', 'schools', 'will', 'remain', 'open', 'for', 'those', 'who', 'need', 'credits', 'to', 'graduate.', 'Cali', 'is', 'broken']\n",
      "['Bed!!!!!...', 'its', 'time,.....', 'hope', 'i', 'go', 'to', 'school', 'tomorrow,', 'all', 'though', 'i', \"don't\", 'feel', 'very', 'well', 'right', 'now']\n",
      "['@onscrn', 'Ahh.', '', '...', 'Well,', 'I', 'was', 'hoping', 'that', 'I', 'could', 'learn', 'some', 'stuff', 'on', 'the', 'way.', '...', 'Why', 'not', 'you', 'and', 'I', 'work', 'on', 'separate', 'things', 'but', 'also']\n",
      "[\"I'm\", 'having', 'a', 'problem', 'with', 'my', 'photo', 'here', 'in', 'twitter', \"amf!!!...can't\", 'see', 'my', 'face!']\n",
      "['@jakeboyd,', 'oh', 'noooo!', '', 'if', 'i', 'blow', 'a', 'tire', \"you're\", 'reaaaally', 'going', 'to', 'have', 'to', 'send', 'up', 'some', 'batman', 'smoke.']\n",
      "['wnna', 'take', 'a', 'bath!!!!']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>c l l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>f r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>w l l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment content\n",
       "0       empty        \n",
       "1     sadness   c l l\n",
       "2     sadness     f r\n",
       "3  enthusiasm       n\n",
       "4     neutral   w l l"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(normalize(data[\"content\"].values[0]))\n",
    "# print(\".....\")\n",
    "# print(data[\"content\"].values[0])\n",
    "\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: normalize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>c l l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>f r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>w l l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment content\n",
       "0       empty        \n",
       "1     sadness   c l l\n",
       "2     sadness     f r\n",
       "3  enthusiasm       n\n",
       "4     neutral   w l l"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes w/ TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.27355555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "text_tf = tf.fit_transform(data['content'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, data['sentiment'], test_size=0.3, random_state=123)\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize each tweet to be an array of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.apply(lambda row: word_tokenize(row['content']), axis=1)\n",
    "X_train = X_train[0:-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 10,  3,  8, 12, 11,  7,  4,  6,  5,  1,  9,  0])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding outcome classes\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(data['sentiment'].unique())\n",
    "output_classes = label_encoder.transform(data['sentiment'].unique()) \n",
    "output_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tiffanylue',\n",
       "  'know',\n",
       "  'listenin',\n",
       "  'bad',\n",
       "  'habit',\n",
       "  'earlier',\n",
       "  'started',\n",
       "  'freakin',\n",
       "  'part'],\n",
       " ['layin', 'n', 'bed', 'headache', 'ughhhh', 'waitin', 'call'],\n",
       " ['funeral', 'ceremony', 'gloomy', 'friday'],\n",
       " ['wants', 'hang', 'friends', 'soon'],\n",
       " ['dannycastillo', 'want', 'trade', 'someone', 'houston', 'tickets', 'one']]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing ---> Should I lemmantize or just stem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the assumption that the features are conditionally independent given an output class. This assumption is most likely not true — hence the name naive Bayes classifier, but the classifier nonetheless performs well in most situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "# y_train = data[\"sentiment\"][0:-20]\n",
    "# train_tfidf = train_df['text'].values.tolist()\n",
    "# test_tfidf = test_df['text'].values.tolist()\n",
    "\n",
    "# NBclassifier = Pipeline([('vect', CountVectorizer()),\n",
    "#                       ('tfidf', TfidfTransformer()),\n",
    "#                       ('clf', MultinomialNB()),\n",
    "# ])\n",
    "\n",
    "# X_train\n",
    "# y_train\n",
    "# NBclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict training set:\n",
    "# dtrain_predictions = NBclassifier.predict_proba(X_train)\n",
    "\n",
    "# Predict testing set:\n",
    "# y_pred_proba = NBclassifier.predict_proba(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just learning, no transformation\n",
    "# classifier = MultinomialNB().fit(X_train_NB, output_classes)\n",
    "# predicted = classifier.predict(X_train_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer will convert the tweets to sequences so that they can be passed through embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_str = 'This is a basic string to see how keras tokenizer works, very basic very'\n",
    "# list_test = basic_str.split(' ')\n",
    "# # list_test\n",
    "\n",
    "# tk = Tokenizer(lower = True, filters='')\n",
    "# tk.fit_on_texts(list_test)\n",
    "# train_tokenized = tk.texts_to_sequences(list_test)\n",
    "# train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(X_train)\n",
    "# test_tokenized = tk.texts_to_sequences(test['tweet'])\n",
    "\n",
    "max_len = 100\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "\n",
    "\n",
    "# labels = to_categorical(np.asarray(data['sentiment'].unique()))\n",
    "\n",
    "# X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "**texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 10, 10, ...,  4,  8,  7])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "X_test = X_train[-20]\n",
    "# y_train = label_encoder.transform(y_train)\n",
    "\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "# y_train\n",
    "# X_test\n",
    "# test_tokenized = tk.texts_to_sequences(X_test)\n",
    "# max_len = 100\n",
    "# X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "\n",
    "\n",
    "# nb_clf.predict(X_test)\n",
    "# X_test\n",
    "\n",
    "# y_train\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Glove embeddings into dictionary [MEMORY ISSUE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "file = open('../data/glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "for line in file:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "'''\n",
    "Create a matrix of one embedding for each word in the training dataset. \n",
    "We can do that by enumerating all unique words in the Tokenizer.word_index \n",
    "and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "'''\n",
    "\n",
    "vocab_size = len(tk.word_index) + 1\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tk.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tk.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
    "\n",
    "We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          10700     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                130013    \n",
      "=================================================================\n",
      "Total params: 140,713\n",
      "Trainable params: 130,013\n",
      "Non-trainable params: 10,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (13,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-3e54201a329f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (13,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# Want to add Stacked LSTM\n",
    "\n",
    "# Added Recurrent single LSTM layer, \n",
    "# model.add(LSTM(64, return_sequences=False,dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "\n",
    "# Init model\n",
    "model = Sequential()\n",
    "# Embedding layer using Glove Pretrained weights\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "# Flattening to conncet to Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(X_train, output_classes, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Notes\n",
    "* Need to remove mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Medium Research Notes:\n",
    "\n",
    "If we need to create a embedding mapping for words with training, how does this affect how we embed the test data (streamed tweets)? Maybe just use Glove.\n",
    "\n",
    "Shall we use Embedding layer as our input layer?\n",
    "\n",
    "Preprocessing\n",
    "KD article\n",
    "Padding\n",
    "Remove stop words\n",
    "\n",
    "\n",
    "CNN training is 1.5 times faster than training LSTMs\n",
    "\n",
    "Need to determine the unique vocabulary of training in order to determine embedding vector length, then I think you pad off of that? Not sure about the padding preprocessing step.\n",
    "\n",
    "Pathtoword embeddings is the path where we’ve  downloaded word embeddings via glove or number batch\n",
    "\n",
    "Is it cheating if I understand all the processing of what this article is doing, and add my own preprocessing, and maybe additional steps? \n",
    "https://medium.com/@panghalarsh/sentiment-analysis-in-python-using-keras-glove-twitter-word-embeddings-and-deep-rnn-on-a-combined-580646cb900a\n",
    "\n",
    "Word Embeddings is better than count vectorizers that one hot encodes sentences because the bagofwords model will not scale well to large datasets due to create sparse vectors in high dimensionality. Also we lose semantics through BagofWords approach. Word embeddings are dense vectors with much lower dimensionality. Secondly, the semantic relationships between words are reflected in the distance and direction of the vectors.\n",
    "\n",
    "Should read into each Keras library being utilized and understanding \n",
    "\n",
    "\n",
    "Need to encode output classes (emotions) w/ LabelEncoders\n",
    "\n",
    "Why do we need to Flatten after we do embedding? Like what does that do?\n",
    "\n",
    "In the Embedding layer (which is layer 0 here) we set the weights for the words to those found in the GloVe word embeddings. By setting trainable to False we make sure that the GloVe word embeddings cannot be changed.\n",
    "\n",
    "The best result is achieved with 100-dimensional word embeddings that are trained on the available data. By doing this, we do not take into account the relationships between the words in the tweet. This can be achieved with a recurrent neural network or a 1D convolutional network. But that’s something for a future post.\n",
    "— Bert Carremans \n",
    "\n",
    "\n",
    "Do we need padding when utilizing word embeddings? I saw some Word2Vec tutorials using padding one hot encoded sequences, but my understanding was that we don’t need it for embeddings.\n",
    "https://medium.com/datadriveninvestor/sentiment-analysis-using-embeddings-f3dd99aeaade\n",
    "\n",
    "\n",
    "Why train my own word embeddings at all? Why not always use Glove? It seems to be a better trained for my model’s embedding layer anyways.\n",
    "\n",
    "\n",
    "There are so many state of the art embedding techniques like SkipGrams, FastText, and ELMo. I should investigate these for their result to text summarizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 1 2 3 6]]\n",
      "[[[ 0.01658997 -0.03503971 -0.03524492]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [ 0.02627129 -0.02832104  0.01373578]]\n",
      "\n",
      " [[-0.03747336 -0.02579566  0.03765706]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [-0.04258297  0.01286627  0.01142017]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "model = Sequential()\n",
    "model.add(Embedding(7, 3, input_length=5))\n",
    "# model.add(Flatten())\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.array([[0, 1, 2, 3, 4], [5, 1, 2, 3, 6]])\n",
    "print(input_array)\n",
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ELMo's pretrained model\n",
    "* When we import this model, should keep it trainable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
