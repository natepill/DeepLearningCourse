{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/natepill/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "# Packages for data preparation\n",
    "# from sklearn.model_selection import train_test_split # Dont need because data folder has already spilt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#For Training Multinomial Naive Bayess\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explains embeddings very well: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "GloVe embeddings: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Notes\n",
    "\n",
    "* After I have a working model, I can concatenate training and test sets because my testing will be with streamed tweets! \n",
    "* There is something wrong with the way that I'm preprocessing tweets, or maybe the word embedding files that I'm using does not contain what I expect\n",
    "* Naive Bayes VS Glove Embeddings w/ LSTM VS Glove Embeddings w/ stacked LSTMs VS Embeddings + CNN VS ELMO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data = pd.read_csv(\"sa-emotions/train_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "\n",
    "data[\"sentiment\"].unique()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data = data[0:50]\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tiffanylue\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "know\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "was\n",
      "<class 'str'>\n",
      "listenin\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "bad\n",
      "<class 'str'>\n",
      "habit\n",
      "<class 'str'>\n",
      "earlier\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "started\n",
      "<class 'str'>\n",
      "freakin\n",
      "<class 'str'>\n",
      "at\n",
      "<class 'str'>\n",
      "his\n",
      "<class 'str'>\n",
      "part\n",
      "<class 'str'>\n",
      "=[\n",
      "<class 'str'>\n",
      "Layin\n",
      "<class 'str'>\n",
      "n\n",
      "<class 'str'>\n",
      "bed\n",
      "<class 'str'>\n",
      "with\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'str'>\n",
      "headache\n",
      "<class 'str'>\n",
      "ughhhh...waitin\n",
      "<class 'str'>\n",
      "on\n",
      "<class 'str'>\n",
      "your\n",
      "<class 'str'>\n",
      "call...\n",
      "<class 'str'>\n",
      "Funeral\n",
      "<class 'str'>\n",
      "ceremony...gloomy\n",
      "<class 'str'>\n",
      "friday...\n",
      "<class 'str'>\n",
      "wants\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "hang\n",
      "<class 'str'>\n",
      "out\n",
      "<class 'str'>\n",
      "with\n",
      "<class 'str'>\n",
      "friends\n",
      "<class 'str'>\n",
      "SOON!\n",
      "<class 'str'>\n",
      "@dannycastillo\n",
      "<class 'str'>\n",
      "We\n",
      "<class 'str'>\n",
      "want\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "trade\n",
      "<class 'str'>\n",
      "with\n",
      "<class 'str'>\n",
      "someone\n",
      "<class 'str'>\n",
      "who\n",
      "<class 'str'>\n",
      "has\n",
      "<class 'str'>\n",
      "Houston\n",
      "<class 'str'>\n",
      "tickets,\n",
      "<class 'str'>\n",
      "but\n",
      "<class 'str'>\n",
      "no\n",
      "<class 'str'>\n",
      "one\n",
      "<class 'str'>\n",
      "will.\n",
      "<class 'str'>\n",
      "Re-pinging\n",
      "<class 'str'>\n",
      "@ghostridah14:\n",
      "<class 'str'>\n",
      "why\n",
      "<class 'str'>\n",
      "didn't\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "go\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "prom?\n",
      "<class 'str'>\n",
      "BC\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "bf\n",
      "<class 'str'>\n",
      "didn't\n",
      "<class 'str'>\n",
      "like\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "friends\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "should\n",
      "<class 'str'>\n",
      "be\n",
      "<class 'str'>\n",
      "sleep,\n",
      "<class 'str'>\n",
      "but\n",
      "<class 'str'>\n",
      "im\n",
      "<class 'str'>\n",
      "not!\n",
      "<class 'str'>\n",
      "thinking\n",
      "<class 'str'>\n",
      "about\n",
      "<class 'str'>\n",
      "an\n",
      "<class 'str'>\n",
      "old\n",
      "<class 'str'>\n",
      "friend\n",
      "<class 'str'>\n",
      "who\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "want.\n",
      "<class 'str'>\n",
      "but\n",
      "<class 'str'>\n",
      "he's\n",
      "<class 'str'>\n",
      "married\n",
      "<class 'str'>\n",
      "now.\n",
      "<class 'str'>\n",
      "damn,\n",
      "<class 'str'>\n",
      "&amp;\n",
      "<class 'str'>\n",
      "he\n",
      "<class 'str'>\n",
      "wants\n",
      "<class 'str'>\n",
      "me\n",
      "<class 'str'>\n",
      "2!\n",
      "<class 'str'>\n",
      "scandalous!\n",
      "<class 'str'>\n",
      "Hmmm.\n",
      "<class 'str'>\n",
      "http://www.djhero.com/\n",
      "<class 'str'>\n",
      "is\n",
      "<class 'str'>\n",
      "down\n",
      "<class 'str'>\n",
      "@charviray\n",
      "<class 'str'>\n",
      "Charlene\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "love.\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "miss\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "@kelcouch\n",
      "<class 'str'>\n",
      "I'm\n",
      "<class 'str'>\n",
      "sorry\n",
      "<class 'str'>\n",
      "at\n",
      "<class 'str'>\n",
      "least\n",
      "<class 'str'>\n",
      "it's\n",
      "<class 'str'>\n",
      "Friday?\n",
      "<class 'str'>\n",
      "cant\n",
      "<class 'str'>\n",
      "fall\n",
      "<class 'str'>\n",
      "asleep\n",
      "<class 'str'>\n",
      "Choked\n",
      "<class 'str'>\n",
      "on\n",
      "<class 'str'>\n",
      "her\n",
      "<class 'str'>\n",
      "retainers\n",
      "<class 'str'>\n",
      "Ugh!\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "have\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "beat\n",
      "<class 'str'>\n",
      "this\n",
      "<class 'str'>\n",
      "stupid\n",
      "<class 'str'>\n",
      "song\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "get\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "next\n",
      "<class 'str'>\n",
      "rude!\n",
      "<class 'str'>\n",
      "@BrodyJenner\n",
      "<class 'str'>\n",
      "if\n",
      "<class 'str'>\n",
      "u\n",
      "<class 'str'>\n",
      "watch\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "hills\n",
      "<class 'str'>\n",
      "in\n",
      "<class 'str'>\n",
      "london\n",
      "<class 'str'>\n",
      "u\n",
      "<class 'str'>\n",
      "will\n",
      "<class 'str'>\n",
      "realise\n",
      "<class 'str'>\n",
      "what\n",
      "<class 'str'>\n",
      "tourture\n",
      "<class 'str'>\n",
      "it\n",
      "<class 'str'>\n",
      "is\n",
      "<class 'str'>\n",
      "because\n",
      "<class 'str'>\n",
      "were\n",
      "<class 'str'>\n",
      "weeks\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "weeks\n",
      "<class 'str'>\n",
      "late\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "just\n",
      "<class 'str'>\n",
      "watch\n",
      "<class 'str'>\n",
      "itonlinelol\n",
      "<class 'str'>\n",
      "Got\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "news\n",
      "<class 'str'>\n",
      "The\n",
      "<class 'str'>\n",
      "storm\n",
      "<class 'str'>\n",
      "is\n",
      "<class 'str'>\n",
      "here\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "electricity\n",
      "<class 'str'>\n",
      "is\n",
      "<class 'str'>\n",
      "gone\n",
      "<class 'str'>\n",
      "@annarosekerr\n",
      "<class 'str'>\n",
      "agreed\n",
      "<class 'str'>\n",
      "So\n",
      "<class 'str'>\n",
      "sleepy\n",
      "<class 'str'>\n",
      "again\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "it's\n",
      "<class 'str'>\n",
      "not\n",
      "<class 'str'>\n",
      "even\n",
      "<class 'str'>\n",
      "that\n",
      "<class 'str'>\n",
      "late.\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "fail\n",
      "<class 'str'>\n",
      "once\n",
      "<class 'str'>\n",
      "again.\n",
      "<class 'str'>\n",
      "@PerezHilton\n",
      "<class 'str'>\n",
      "lady\n",
      "<class 'str'>\n",
      "gaga\n",
      "<class 'str'>\n",
      "tweeted\n",
      "<class 'str'>\n",
      "about\n",
      "<class 'str'>\n",
      "not\n",
      "<class 'str'>\n",
      "being\n",
      "<class 'str'>\n",
      "impressed\n",
      "<class 'str'>\n",
      "by\n",
      "<class 'str'>\n",
      "her\n",
      "<class 'str'>\n",
      "video\n",
      "<class 'str'>\n",
      "leaking\n",
      "<class 'str'>\n",
      "just\n",
      "<class 'str'>\n",
      "so\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "know\n",
      "<class 'str'>\n",
      "How\n",
      "<class 'str'>\n",
      "are\n",
      "<class 'str'>\n",
      "YOU\n",
      "<class 'str'>\n",
      "convinced\n",
      "<class 'str'>\n",
      "that\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "have\n",
      "<class 'str'>\n",
      "always\n",
      "<class 'str'>\n",
      "wanted\n",
      "<class 'str'>\n",
      "you?\n",
      "<class 'str'>\n",
      "What\n",
      "<class 'str'>\n",
      "signals\n",
      "<class 'str'>\n",
      "did\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "give\n",
      "<class 'str'>\n",
      "off...damn\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "think\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "just\n",
      "<class 'str'>\n",
      "lost\n",
      "<class 'str'>\n",
      "another\n",
      "<class 'str'>\n",
      "friend\n",
      "<class 'str'>\n",
      "@raaaaaaek\n",
      "<class 'str'>\n",
      "oh\n",
      "<class 'str'>\n",
      "too\n",
      "<class 'str'>\n",
      "bad!\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "hope\n",
      "<class 'str'>\n",
      "it\n",
      "<class 'str'>\n",
      "gets\n",
      "<class 'str'>\n",
      "better.\n",
      "<class 'str'>\n",
      "I've\n",
      "<class 'str'>\n",
      "been\n",
      "<class 'str'>\n",
      "having\n",
      "<class 'str'>\n",
      "sleep\n",
      "<class 'str'>\n",
      "issues\n",
      "<class 'str'>\n",
      "lately\n",
      "<class 'str'>\n",
      "too\n",
      "<class 'str'>\n",
      "Wondering\n",
      "<class 'str'>\n",
      "why\n",
      "<class 'str'>\n",
      "I'm\n",
      "<class 'str'>\n",
      "awake\n",
      "<class 'str'>\n",
      "at\n",
      "<class 'str'>\n",
      "7am,writing\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'str'>\n",
      "new\n",
      "<class 'str'>\n",
      "song,plotting\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "evil\n",
      "<class 'str'>\n",
      "secret\n",
      "<class 'str'>\n",
      "plots\n",
      "<class 'str'>\n",
      "muahahaha...oh\n",
      "<class 'str'>\n",
      "damn\n",
      "<class 'str'>\n",
      "it,not\n",
      "<class 'str'>\n",
      "secret\n",
      "<class 'str'>\n",
      "anymore\n",
      "<class 'str'>\n",
      "No\n",
      "<class 'str'>\n",
      "Topic\n",
      "<class 'str'>\n",
      "Maps\n",
      "<class 'str'>\n",
      "talks\n",
      "<class 'str'>\n",
      "at\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "Balisage\n",
      "<class 'str'>\n",
      "Markup\n",
      "<class 'str'>\n",
      "Conference\n",
      "<class 'str'>\n",
      "2009\n",
      "<class 'str'>\n",
      "Program\n",
      "<class 'str'>\n",
      "online\n",
      "<class 'str'>\n",
      "at\n",
      "<class 'str'>\n",
      "http://tr.im/mL6Z\n",
      "<class 'str'>\n",
      "(via\n",
      "<class 'str'>\n",
      "@bobdc)\n",
      "<class 'str'>\n",
      "#topicmaps\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "ate\n",
      "<class 'str'>\n",
      "Something\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "don't\n",
      "<class 'str'>\n",
      "know\n",
      "<class 'str'>\n",
      "what\n",
      "<class 'str'>\n",
      "it\n",
      "<class 'str'>\n",
      "is...\n",
      "<class 'str'>\n",
      "Why\n",
      "<class 'str'>\n",
      "do\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "keep\n",
      "<class 'str'>\n",
      "Telling\n",
      "<class 'str'>\n",
      "things\n",
      "<class 'str'>\n",
      "about\n",
      "<class 'str'>\n",
      "food\n",
      "<class 'str'>\n",
      "so\n",
      "<class 'str'>\n",
      "tired\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "think\n",
      "<class 'str'>\n",
      "i'm\n",
      "<class 'str'>\n",
      "definitely\n",
      "<class 'str'>\n",
      "going\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "get\n",
      "<class 'str'>\n",
      "an\n",
      "<class 'str'>\n",
      "ear\n",
      "<class 'str'>\n",
      "infection.\n",
      "<class 'str'>\n",
      "going\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "bed\n",
      "<class 'str'>\n",
      "&quot;early&quot;\n",
      "<class 'str'>\n",
      "for\n",
      "<class 'str'>\n",
      "once.\n",
      "<class 'str'>\n",
      "On\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "way\n",
      "<class 'str'>\n",
      "home\n",
      "<class 'str'>\n",
      "n\n",
      "<class 'str'>\n",
      "having\n",
      "<class 'str'>\n",
      "2\n",
      "<class 'str'>\n",
      "deal\n",
      "<class 'str'>\n",
      "w\n",
      "<class 'str'>\n",
      "underage\n",
      "<class 'str'>\n",
      "girls\n",
      "<class 'str'>\n",
      "drinking\n",
      "<class 'str'>\n",
      "gin\n",
      "<class 'str'>\n",
      "on\n",
      "<class 'str'>\n",
      "da\n",
      "<class 'str'>\n",
      "bus\n",
      "<class 'str'>\n",
      "while\n",
      "<class 'str'>\n",
      "talking\n",
      "<class 'str'>\n",
      "bout\n",
      "<class 'str'>\n",
      "keggers......damn\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "feel\n",
      "<class 'str'>\n",
      "old\n",
      "<class 'str'>\n",
      "@IsaacMascote\n",
      "<class 'str'>\n",
      "i'm\n",
      "<class 'str'>\n",
      "sorry\n",
      "<class 'str'>\n",
      "people\n",
      "<class 'str'>\n",
      "are\n",
      "<class 'str'>\n",
      "so\n",
      "<class 'str'>\n",
      "rude\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "you,\n",
      "<class 'str'>\n",
      "isaac,\n",
      "<class 'str'>\n",
      "they\n",
      "<class 'str'>\n",
      "should\n",
      "<class 'str'>\n",
      "get\n",
      "<class 'str'>\n",
      "some\n",
      "<class 'str'>\n",
      "manners\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "know\n",
      "<class 'str'>\n",
      "better\n",
      "<class 'str'>\n",
      "than\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "be\n",
      "<class 'str'>\n",
      "so\n",
      "<class 'str'>\n",
      "lewd!\n",
      "<class 'str'>\n",
      "Damm\n",
      "<class 'str'>\n",
      "servers\n",
      "<class 'str'>\n",
      "still\n",
      "<class 'str'>\n",
      "down\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "need\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "hit\n",
      "<class 'str'>\n",
      "80\n",
      "<class 'str'>\n",
      "before\n",
      "<class 'str'>\n",
      "all\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "koxpers\n",
      "<class 'str'>\n",
      "pass\n",
      "<class 'str'>\n",
      "me\n",
      "<class 'str'>\n",
      "Fudge....\n",
      "<class 'str'>\n",
      "Just\n",
      "<class 'str'>\n",
      "BS'd\n",
      "<class 'str'>\n",
      "that\n",
      "<class 'str'>\n",
      "whole\n",
      "<class 'str'>\n",
      "paper....\n",
      "<class 'str'>\n",
      "So\n",
      "<class 'str'>\n",
      "tired....\n",
      "<class 'str'>\n",
      "Ugh\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "hate\n",
      "<class 'str'>\n",
      "school.....\n",
      "<class 'str'>\n",
      "time\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "sleep!!!!!!!!!!!\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "HATE\n",
      "<class 'str'>\n",
      "CANCER.\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "HATE\n",
      "<class 'str'>\n",
      "IT\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "HATE\n",
      "<class 'str'>\n",
      "IT\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "HATE\n",
      "<class 'str'>\n",
      "IT.\n",
      "<class 'str'>\n",
      "It\n",
      "<class 'str'>\n",
      "is\n",
      "<class 'str'>\n",
      "so\n",
      "<class 'str'>\n",
      "annoying\n",
      "<class 'str'>\n",
      "when\n",
      "<class 'str'>\n",
      "she\n",
      "<class 'str'>\n",
      "starts\n",
      "<class 'str'>\n",
      "typing\n",
      "<class 'str'>\n",
      "on\n",
      "<class 'str'>\n",
      "her\n",
      "<class 'str'>\n",
      "computer\n",
      "<class 'str'>\n",
      "in\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "middle\n",
      "<class 'str'>\n",
      "of\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "night!\n",
      "<class 'str'>\n",
      "@cynthia_123\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "cant\n",
      "<class 'str'>\n",
      "sleep\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "<class 'str'>\n",
      "missed\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "bl***y\n",
      "<class 'str'>\n",
      "bus!!!!!!!!\n",
      "<class 'str'>\n",
      "feels\n",
      "<class 'str'>\n",
      "strong\n",
      "<class 'str'>\n",
      "contractions\n",
      "<class 'str'>\n",
      "but\n",
      "<class 'str'>\n",
      "wants\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "go\n",
      "<class 'str'>\n",
      "out.\n",
      "<class 'str'>\n",
      "http://plurk.com/p/wxidk\n",
      "<class 'str'>\n",
      "SoCal!\n",
      "<class 'str'>\n",
      "stoked.\n",
      "<class 'str'>\n",
      "or\n",
      "<class 'str'>\n",
      "maybe\n",
      "<class 'str'>\n",
      "not..\n",
      "<class 'str'>\n",
      "tomorrow\n",
      "<class 'str'>\n",
      "Screw\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "@davidbrussee!\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "only\n",
      "<class 'str'>\n",
      "have\n",
      "<class 'str'>\n",
      "3\n",
      "<class 'str'>\n",
      "weeks...\n",
      "<class 'str'>\n",
      "@ether_radio\n",
      "<class 'str'>\n",
      "yeah\n",
      "<class 'str'>\n",
      ":S\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "feel\n",
      "<class 'str'>\n",
      "all\n",
      "<class 'str'>\n",
      "funny\n",
      "<class 'str'>\n",
      "cause\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "haven't\n",
      "<class 'str'>\n",
      "slept\n",
      "<class 'str'>\n",
      "enough\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "woke\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "mum\n",
      "<class 'str'>\n",
      "up\n",
      "<class 'str'>\n",
      "cause\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "was\n",
      "<class 'str'>\n",
      "singing\n",
      "<class 'str'>\n",
      "she's\n",
      "<class 'str'>\n",
      "not\n",
      "<class 'str'>\n",
      "impressed\n",
      "<class 'str'>\n",
      ":S\n",
      "<class 'str'>\n",
      "you?\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "need\n",
      "<class 'str'>\n",
      "skott\n",
      "<class 'str'>\n",
      "right\n",
      "<class 'str'>\n",
      "now\n",
      "<class 'str'>\n",
      "has\n",
      "<class 'str'>\n",
      "work\n",
      "<class 'str'>\n",
      "this\n",
      "<class 'str'>\n",
      "afternoon\n",
      "<class 'str'>\n",
      "@GABBYiSACTiVE\n",
      "<class 'str'>\n",
      "Aw\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "would\n",
      "<class 'str'>\n",
      "not\n",
      "<class 'str'>\n",
      "unfollow\n",
      "<class 'str'>\n",
      "me\n",
      "<class 'str'>\n",
      "would\n",
      "<class 'str'>\n",
      "you?\n",
      "<class 'str'>\n",
      "Then\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "would\n",
      "<class 'str'>\n",
      "cry\n",
      "<class 'str'>\n",
      "mmm\n",
      "<class 'str'>\n",
      "much\n",
      "<class 'str'>\n",
      "better\n",
      "<class 'str'>\n",
      "day...\n",
      "<class 'str'>\n",
      "so\n",
      "<class 'str'>\n",
      "far!\n",
      "<class 'str'>\n",
      "it's\n",
      "<class 'str'>\n",
      "still\n",
      "<class 'str'>\n",
      "quite\n",
      "<class 'str'>\n",
      "early.\n",
      "<class 'str'>\n",
      "last\n",
      "<class 'str'>\n",
      "day\n",
      "<class 'str'>\n",
      "of\n",
      "<class 'str'>\n",
      "#uds\n",
      "<class 'str'>\n",
      "@DavidArchie\n",
      "<class 'str'>\n",
      "&lt;3\n",
      "<class 'str'>\n",
      "your\n",
      "<class 'str'>\n",
      "gonna\n",
      "<class 'str'>\n",
      "be\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "first\n",
      "<class 'str'>\n",
      "twitter\n",
      "<class 'str'>\n",
      ";)\n",
      "<class 'str'>\n",
      "cause\n",
      "<class 'str'>\n",
      "your\n",
      "<class 'str'>\n",
      "amazing\n",
      "<class 'str'>\n",
      "lol.\n",
      "<class 'str'>\n",
      "come\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "canada\n",
      "<class 'str'>\n",
      "would\n",
      "<class 'str'>\n",
      "do\n",
      "<class 'str'>\n",
      "anything\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "see\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "perform\n",
      "<class 'str'>\n",
      "just\n",
      "<class 'str'>\n",
      "picked\n",
      "<class 'str'>\n",
      "up\n",
      "<class 'str'>\n",
      "her\n",
      "<class 'str'>\n",
      "Blackberry\n",
      "<class 'str'>\n",
      "from\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "middle\n",
      "<class 'str'>\n",
      "of\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "street!\n",
      "<class 'str'>\n",
      "Both\n",
      "<class 'str'>\n",
      "she\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "it\n",
      "<class 'str'>\n",
      "are\n",
      "<class 'str'>\n",
      "crushed!\n",
      "<class 'str'>\n",
      "Why\n",
      "<class 'str'>\n",
      "do\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "have\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "feeling\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "should\n",
      "<class 'str'>\n",
      "be\n",
      "<class 'str'>\n",
      "packing\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "hitting\n",
      "<class 'str'>\n",
      "for\n",
      "<class 'str'>\n",
      "SFO\n",
      "<class 'str'>\n",
      "around\n",
      "<class 'str'>\n",
      "this\n",
      "<class 'str'>\n",
      "time\n",
      "<class 'str'>\n",
      "of\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "year?\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "think\n",
      "<class 'str'>\n",
      "I'm\n",
      "<class 'str'>\n",
      "missing\n",
      "<class 'str'>\n",
      "something...\n",
      "<class 'str'>\n",
      "@creyes\n",
      "<class 'str'>\n",
      "middle\n",
      "<class 'str'>\n",
      "school\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "elem.\n",
      "<class 'str'>\n",
      "High\n",
      "<class 'str'>\n",
      "schools\n",
      "<class 'str'>\n",
      "will\n",
      "<class 'str'>\n",
      "remain\n",
      "<class 'str'>\n",
      "open\n",
      "<class 'str'>\n",
      "for\n",
      "<class 'str'>\n",
      "those\n",
      "<class 'str'>\n",
      "who\n",
      "<class 'str'>\n",
      "need\n",
      "<class 'str'>\n",
      "credits\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "graduate.\n",
      "<class 'str'>\n",
      "Cali\n",
      "<class 'str'>\n",
      "is\n",
      "<class 'str'>\n",
      "broken\n",
      "<class 'str'>\n",
      "Bed!!!!!...\n",
      "<class 'str'>\n",
      "its\n",
      "<class 'str'>\n",
      "time,.....\n",
      "<class 'str'>\n",
      "hope\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "go\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "school\n",
      "<class 'str'>\n",
      "tomorrow,\n",
      "<class 'str'>\n",
      "all\n",
      "<class 'str'>\n",
      "though\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "don't\n",
      "<class 'str'>\n",
      "feel\n",
      "<class 'str'>\n",
      "very\n",
      "<class 'str'>\n",
      "well\n",
      "<class 'str'>\n",
      "right\n",
      "<class 'str'>\n",
      "now\n",
      "<class 'str'>\n",
      "@onscrn\n",
      "<class 'str'>\n",
      "Ahh.\n",
      "<class 'str'>\n",
      "...\n",
      "<class 'str'>\n",
      "Well,\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "was\n",
      "<class 'str'>\n",
      "hoping\n",
      "<class 'str'>\n",
      "that\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "could\n",
      "<class 'str'>\n",
      "learn\n",
      "<class 'str'>\n",
      "some\n",
      "<class 'str'>\n",
      "stuff\n",
      "<class 'str'>\n",
      "on\n",
      "<class 'str'>\n",
      "the\n",
      "<class 'str'>\n",
      "way.\n",
      "<class 'str'>\n",
      "...\n",
      "<class 'str'>\n",
      "Why\n",
      "<class 'str'>\n",
      "not\n",
      "<class 'str'>\n",
      "you\n",
      "<class 'str'>\n",
      "and\n",
      "<class 'str'>\n",
      "I\n",
      "<class 'str'>\n",
      "work\n",
      "<class 'str'>\n",
      "on\n",
      "<class 'str'>\n",
      "separate\n",
      "<class 'str'>\n",
      "things\n",
      "<class 'str'>\n",
      "but\n",
      "<class 'str'>\n",
      "also\n",
      "<class 'str'>\n",
      "I'm\n",
      "<class 'str'>\n",
      "having\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'str'>\n",
      "problem\n",
      "<class 'str'>\n",
      "with\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "photo\n",
      "<class 'str'>\n",
      "here\n",
      "<class 'str'>\n",
      "in\n",
      "<class 'str'>\n",
      "twitter\n",
      "<class 'str'>\n",
      "amf!!!...can't\n",
      "<class 'str'>\n",
      "see\n",
      "<class 'str'>\n",
      "my\n",
      "<class 'str'>\n",
      "face!\n",
      "<class 'str'>\n",
      "@jakeboyd,\n",
      "<class 'str'>\n",
      "oh\n",
      "<class 'str'>\n",
      "noooo!\n",
      "<class 'str'>\n",
      "if\n",
      "<class 'str'>\n",
      "i\n",
      "<class 'str'>\n",
      "blow\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'str'>\n",
      "tire\n",
      "<class 'str'>\n",
      "you're\n",
      "<class 'str'>\n",
      "reaaaally\n",
      "<class 'str'>\n",
      "going\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "have\n",
      "<class 'str'>\n",
      "to\n",
      "<class 'str'>\n",
      "send\n",
      "<class 'str'>\n",
      "up\n",
      "<class 'str'>\n",
      "some\n",
      "<class 'str'>\n",
      "batman\n",
      "<class 'str'>\n",
      "smoke.\n",
      "<class 'str'>\n",
      "wnna\n",
      "<class 'str'>\n",
      "take\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'str'>\n",
      "bath!!!!\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "wnnkebh is an unknown string function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-b2f749d5459f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;31m# X_train = [normalize(tweet) for tweet in X_train]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3571\u001b[0m         \u001b[0;31m# if we are a string, try to dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3573\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_aggregate_string_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3575\u001b[0m         \u001b[0;31m# handle ufuncs and lambdas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_try_aggregate_string_function\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{arg} is an unknown string function\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: wnnkebh is an unknown string function"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import nltk\n",
    "import inflect\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "Look at Emlo article for preprocessing w/ lambda\n",
    "Do lemmantizing, not stemming?? Why not use all normalizing techniques?\n",
    "'''\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "    new_text = ''\n",
    "#     print(words)\n",
    "#     print(type(words))\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        print(type(word))\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#         new_words.append(new_word)\n",
    "        new_text += new_word\n",
    "    return new_text\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "    new_text = ''\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "#         new_words.append(new_word)\n",
    "        new_text += new_word\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "# By default, all punctuation is removed, turning the texts into space-separated sequences of words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "    new_text = ''\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_text += new_word\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "#     new_words = []\n",
    "    new_text = ''\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_text += new_word\n",
    "        else:\n",
    "            new_text += word\n",
    "            \n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "    new_text = ''\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_text += word\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    new_text = ''\n",
    "#     stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        new_text += stem\n",
    "    return stems\n",
    "\n",
    "\n",
    "def remove_mentions(words):\n",
    "    ''' Remove @ mentions'''\n",
    "    new_text = ''\n",
    "    \n",
    "    for word in words:\n",
    "        new_text += re.sub(r'@[A-Za-z0-9]+','', words)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    \n",
    "    for words in words.str.split():\n",
    "\n",
    "        words = remove_non_ascii(words)\n",
    "        words = to_lowercase(words)\n",
    "        words = remove_punctuation(words)\n",
    "        words = replace_numbers(words)\n",
    "        words = remove_stopwords(words)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "data[\"content\"].apply(normalize(data[\"content\"]))\n",
    "# X_train = [normalize(tweet) for tweet in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>worry</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Got the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sadness</td>\n",
       "      <td>The storm is here and the electricity is gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>@annarosekerr agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sadness</td>\n",
       "      <td>So sleepy again and it's not even that late. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>worry</td>\n",
       "      <td>@PerezHilton lady gaga tweeted about not being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sadness</td>\n",
       "      <td>How are YOU convinced that I have always wante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>worry</td>\n",
       "      <td>@raaaaaaek oh too bad! I hope it gets better. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fun</td>\n",
       "      <td>Wondering why I'm awake at 7am,writing a new s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>neutral</td>\n",
       "      <td>No Topic Maps talks at the Balisage Markup Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>worry</td>\n",
       "      <td>I ate Something I don't know what it is... Why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sadness</td>\n",
       "      <td>so tired and i think i'm definitely going to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>worry</td>\n",
       "      <td>On my way home n having 2 deal w underage girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@IsaacMascote  i'm sorry people are so rude to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>worry</td>\n",
       "      <td>Damm servers still down  i need to hit 80 befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Fudge.... Just BS'd that whole paper.... So ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>worry</td>\n",
       "      <td>I HATE CANCER. I HATE IT I HATE IT I HATE IT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hate</td>\n",
       "      <td>It is so annoying when she starts typing on he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@cynthia_123 i cant sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I missed the bl***y bus!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>neutral</td>\n",
       "      <td>feels strong contractions but wants to go out....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>neutral</td>\n",
       "      <td>SoCal!  stoked. or maybe not.. tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Screw you @davidbrussee! I only have 3 weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@ether_radio yeah :S i feel all funny cause i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>worry</td>\n",
       "      <td>I need skott right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>neutral</td>\n",
       "      <td>has work this afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@GABBYiSACTiVE Aw you would not unfollow me wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>happiness</td>\n",
       "      <td>mmm much better day... so far! it's still quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>fun</td>\n",
       "      <td>@DavidArchie &amp;lt;3 your gonna be the first  tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>worry</td>\n",
       "      <td>just picked up her Blackberry from the middle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>worry</td>\n",
       "      <td>Why do I have the feeling I should be packing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>empty</td>\n",
       "      <td>@creyes middle school and elem. High schools w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>worry</td>\n",
       "      <td>Bed!!!!!... its time,..... hope i go to school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>worry</td>\n",
       "      <td>@onscrn Ahh.  ... Well, I was hoping that I co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I'm having a problem with my photo here in twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@jakeboyd, oh noooo!  if i blow a tire you're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>neutral</td>\n",
       "      <td>wnna take a bath!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                            content\n",
       "0        empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1      sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2      sadness                Funeral ceremony...gloomy friday...\n",
       "3   enthusiasm               wants to hang out with friends SOON!\n",
       "4      neutral  @dannycastillo We want to trade with someone w...\n",
       "5        worry  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6      sadness  I should be sleep, but im not! thinking about ...\n",
       "7        worry               Hmmm. http://www.djhero.com/ is down\n",
       "8      sadness            @charviray Charlene my love. I miss you\n",
       "9      sadness         @kelcouch I'm sorry  at least it's Friday?\n",
       "10     neutral                                   cant fall asleep\n",
       "11       worry                            Choked on her retainers\n",
       "12     sadness  Ugh! I have to beat this stupid song to get to...\n",
       "13     sadness  @BrodyJenner if u watch the hills in london u ...\n",
       "14    surprise                                       Got the news\n",
       "15     sadness      The storm is here and the electricity is gone\n",
       "16        love                               @annarosekerr agreed\n",
       "17     sadness  So sleepy again and it's not even that late. I...\n",
       "18       worry  @PerezHilton lady gaga tweeted about not being...\n",
       "19     sadness  How are YOU convinced that I have always wante...\n",
       "20       worry  @raaaaaaek oh too bad! I hope it gets better. ...\n",
       "21         fun  Wondering why I'm awake at 7am,writing a new s...\n",
       "22     neutral  No Topic Maps talks at the Balisage Markup Con...\n",
       "23       worry  I ate Something I don't know what it is... Why...\n",
       "24     sadness  so tired and i think i'm definitely going to g...\n",
       "25       worry  On my way home n having 2 deal w underage girl...\n",
       "26     sadness  @IsaacMascote  i'm sorry people are so rude to...\n",
       "27       worry  Damm servers still down  i need to hit 80 befo...\n",
       "28     sadness  Fudge.... Just BS'd that whole paper.... So ti...\n",
       "29       worry      I HATE CANCER. I HATE IT I HATE IT I HATE IT.\n",
       "30        hate  It is so annoying when she starts typing on he...\n",
       "31     neutral                          @cynthia_123 i cant sleep\n",
       "32     neutral                    I missed the bl***y bus!!!!!!!!\n",
       "33     neutral  feels strong contractions but wants to go out....\n",
       "34     neutral            SoCal!  stoked. or maybe not.. tomorrow\n",
       "35     neutral    Screw you @davidbrussee! I only have 3 weeks...\n",
       "36     sadness  @ether_radio yeah :S i feel all funny cause i ...\n",
       "37       worry                             I need skott right now\n",
       "38     neutral                            has work this afternoon\n",
       "39     neutral  @GABBYiSACTiVE Aw you would not unfollow me wo...\n",
       "40   happiness  mmm much better day... so far! it's still quit...\n",
       "41         fun  @DavidArchie &lt;3 your gonna be the first  tw...\n",
       "42       worry  just picked up her Blackberry from the middle ...\n",
       "43       worry  Why do I have the feeling I should be packing ...\n",
       "44       empty  @creyes middle school and elem. High schools w...\n",
       "45       worry  Bed!!!!!... its time,..... hope i go to school...\n",
       "46       worry  @onscrn Ahh.  ... Well, I was hoping that I co...\n",
       "47     sadness  I'm having a problem with my photo here in twi...\n",
       "48     neutral  @jakeboyd, oh noooo!  if i blow a tire you're ...\n",
       "49     neutral                               wnna take a bath!!!!"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes w/ TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.27355555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "text_tf = tf.fit_transform(data['content'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, data['sentiment'], test_size=0.3, random_state=123)\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize each tweet to be an array of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.apply(lambda row: word_tokenize(row['content']), axis=1)\n",
    "X_train = X_train[0:-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 10,  3,  8, 12, 11,  7,  4,  6,  5,  1,  9,  0])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding outcome classes\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(data['sentiment'].unique())\n",
    "output_classes = label_encoder.transform(data['sentiment'].unique()) \n",
    "output_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tiffanylue',\n",
       "  'know',\n",
       "  'listenin',\n",
       "  'bad',\n",
       "  'habit',\n",
       "  'earlier',\n",
       "  'started',\n",
       "  'freakin',\n",
       "  'part'],\n",
       " ['layin', 'n', 'bed', 'headache', 'ughhhh', 'waitin', 'call'],\n",
       " ['funeral', 'ceremony', 'gloomy', 'friday'],\n",
       " ['wants', 'hang', 'friends', 'soon'],\n",
       " ['dannycastillo', 'want', 'trade', 'someone', 'houston', 'tickets', 'one']]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing ---> Should I lemmantize or just stem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the assumption that the features are conditionally independent given an output class. This assumption is most likely not truehence the name naive Bayes classifier, but the classifier nonetheless performs well in most situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "# y_train = data[\"sentiment\"][0:-20]\n",
    "# train_tfidf = train_df['text'].values.tolist()\n",
    "# test_tfidf = test_df['text'].values.tolist()\n",
    "\n",
    "# NBclassifier = Pipeline([('vect', CountVectorizer()),\n",
    "#                       ('tfidf', TfidfTransformer()),\n",
    "#                       ('clf', MultinomialNB()),\n",
    "# ])\n",
    "\n",
    "# X_train\n",
    "# y_train\n",
    "# NBclassifier.fit(X_train, y_train)\n",
    "\n",
    "#Predict training set:\n",
    "# dtrain_predictions = NBclassifier.predict_proba(X_train)\n",
    "\n",
    "#Predict testing set:\n",
    "# y_pred_proba = NBclassifier.predict_proba(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just learning, no transformation\n",
    "# classifier = MultinomialNB().fit(X_train_NB, output_classes)\n",
    "# predicted = classifier.predict(X_train_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer will convert the tweets to sequences so that they can be passed through embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_str = 'This is a basic string to see how keras tokenizer works, very basic very'\n",
    "# list_test = basic_str.split(' ')\n",
    "# # list_test\n",
    "\n",
    "# tk = Tokenizer(lower = True, filters='')\n",
    "# tk.fit_on_texts(list_test)\n",
    "# train_tokenized = tk.texts_to_sequences(list_test)\n",
    "# train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(X_train)\n",
    "# test_tokenized = tk.texts_to_sequences(test['tweet'])\n",
    "\n",
    "max_len = 100\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "\n",
    "\n",
    "# labels = to_categorical(np.asarray(data['sentiment'].unique()))\n",
    "\n",
    "# X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "**texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 10, 10, ...,  4,  8,  7])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "X_test = X_train[-20]\n",
    "# y_train = label_encoder.transform(y_train)\n",
    "\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "# y_train\n",
    "# X_test\n",
    "# test_tokenized = tk.texts_to_sequences(X_test)\n",
    "# max_len = 100\n",
    "# X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "\n",
    "\n",
    "# nb_clf.predict(X_test)\n",
    "# X_test\n",
    "\n",
    "# y_train\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Glove embeddings into dictionary [MEMORY ISSUE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "file = open('../data/glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "for line in file:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "'''\n",
    "Create a matrix of one embedding for each word in the training dataset. \n",
    "We can do that by enumerating all unique words in the Tokenizer.word_index \n",
    "and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "'''\n",
    "\n",
    "vocab_size = len(tk.word_index) + 1\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tk.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tk.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
    "\n",
    "We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          10700     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                130013    \n",
      "=================================================================\n",
      "Total params: 140,713\n",
      "Trainable params: 130,013\n",
      "Non-trainable params: 10,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (13,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-3e54201a329f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (13,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# Want to add Stacked LSTM\n",
    "\n",
    "# Added Recurrent single LSTM layer, \n",
    "# model.add(LSTM(64, return_sequences=False,dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "\n",
    "# Init model\n",
    "model = Sequential()\n",
    "# Embedding layer using Glove Pretrained weights\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "# Flattening to conncet to Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(X_train, output_classes, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Notes\n",
    "* Need to remove mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Medium Research Notes:\n",
    "\n",
    "If we need to create a embedding mapping for words with training, how does this affect how we embed the test data (streamed tweets)? Maybe just use Glove.\n",
    "\n",
    "Shall we use Embedding layer as our input layer?\n",
    "\n",
    "Preprocessing\n",
    "KD article\n",
    "Padding\n",
    "Remove stop words\n",
    "\n",
    "\n",
    "CNN training is 1.5 times faster than training LSTMs\n",
    "\n",
    "Need to determine the unique vocabulary of training in order to determine embedding vector length, then I think you pad off of that? Not sure about the padding preprocessing step.\n",
    "\n",
    "Pathtoword embeddings is the path where weve  downloaded word embeddings via glove or number batch\n",
    "\n",
    "Is it cheating if I understand all the processing of what this article is doing, and add my own preprocessing, and maybe additional steps? \n",
    "https://medium.com/@panghalarsh/sentiment-analysis-in-python-using-keras-glove-twitter-word-embeddings-and-deep-rnn-on-a-combined-580646cb900a\n",
    "\n",
    "Word Embeddings is better than count vectorizers that one hot encodes sentences because the bagofwords model will not scale well to large datasets due to create sparse vectors in high dimensionality. Also we lose semantics through BagofWords approach. Word embeddings are dense vectors with much lower dimensionality. Secondly, the semantic relationships between words are reflected in the distance and direction of the vectors.\n",
    "\n",
    "Should read into each Keras library being utilized and understanding \n",
    "\n",
    "\n",
    "Need to encode output classes (emotions) w/ LabelEncoders\n",
    "\n",
    "Why do we need to Flatten after we do embedding? Like what does that do?\n",
    "\n",
    "In the Embedding layer (which is layer 0 here) we set the weights for the words to those found in the GloVe word embeddings. By setting trainable to False we make sure that the GloVe word embeddings cannot be changed.\n",
    "\n",
    "The best result is achieved with 100-dimensional word embeddings that are trained on the available data. By doing this, we do not take into account the relationships between the words in the tweet. This can be achieved with a recurrent neural network or a 1D convolutional network. But thats something for a future post.\n",
    " Bert Carremans \n",
    "\n",
    "\n",
    "Do we need padding when utilizing word embeddings? I saw some Word2Vec tutorials using padding one hot encoded sequences, but my understanding was that we dont need it for embeddings.\n",
    "https://medium.com/datadriveninvestor/sentiment-analysis-using-embeddings-f3dd99aeaade\n",
    "\n",
    "\n",
    "Why train my own word embeddings at all? Why not always use Glove? It seems to be a better trained for my models embedding layer anyways.\n",
    "\n",
    "\n",
    "There are so many state of the art embedding techniques like SkipGrams, FastText, and ELMo. I should investigate these for their result to text summarizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 1 2 3 6]]\n",
      "[[[ 0.01658997 -0.03503971 -0.03524492]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [ 0.02627129 -0.02832104  0.01373578]]\n",
      "\n",
      " [[-0.03747336 -0.02579566  0.03765706]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [-0.04258297  0.01286627  0.01142017]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "model = Sequential()\n",
    "model.add(Embedding(7, 3, input_length=5))\n",
    "# model.add(Flatten())\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.array([[0, 1, 2, 3, 4], [5, 1, 2, 3, 6]])\n",
    "print(input_array)\n",
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ELMo's pretrained model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No need "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
