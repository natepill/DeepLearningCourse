{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from textblob import TextBlob\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "#For Training Multinomial Naive Bayess\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explains embeddings very well: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "GloVe embeddings: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Notes and Presentation Notes\n",
    "\n",
    "* After I have a working model, I can concatenate training and test sets because my testing will be with streamed tweets! \n",
    "* There is something wrong with the way that I'm preprocessing tweets, or maybe the word embedding files that I'm using does not contain what I expect\n",
    "* Naive Bayes VS Glove Embeddings w/ LSTM VS Glove Embeddings w/ stacked LSTMs VS Embeddings + CNN VS ELMO\n",
    "* When we incorporate backend use the following article for preprocessing:\n",
    "* TensorBoard for callback optimization\n",
    "* GridSearch for parameter tuning (best dimensions for embeddings, epochs, etc) ---> save best model\n",
    "\n",
    "\n",
    "\n",
    "* Great for explaining Word EMbeddings:\n",
    "\n",
    "https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf?gi=a2d3e63ba57a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "original_data = pd.read_csv(\"sa-emotions/train_data.csv\")\n",
    "\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with smaller sample of data and split sentences into array of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data = original_data[0:50]\n",
    "\n",
    "data[\"content\"] = data.apply(lambda row: word_tokenize(row['content']), axis=1)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def spell_correct(words):\n",
    "    \"\"\" Fix spelling corrections \"\"\"\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        corrected_words.append(TextBlob(word).correct().raw)\n",
    "\n",
    "    return corrected_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = spell_correct(words)\n",
    "    return words\n",
    "\n",
    "# words = normalize(X_train)\n",
    "data[\"content\"] = data[\"content\"].apply(lambda x: \" \".join(normalize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue know listening bad habit earlier st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>laying n bed headache ughhhh waiting call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want hang friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someone houston ticke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  tiffanylue know listening bad habit earlier st...\n",
       "1     sadness          laying n bed headache ughhhh waiting call\n",
       "2     sadness                     funeral ceremony gloomy friday\n",
       "3  enthusiasm                             want hang friends soon\n",
       "4     neutral  dannycastillo want trade someone houston ticke..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"content\"], data['sentiment'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes w/ TFIDF\n",
    "* We make the assumption that the features are conditionally independent given an output class. This assumption is most likely not true — hence the name naive Bayes classifier, but the classifier nonetheless performs well in most situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "text_tf = tf.fit_transform(data['content'])\n",
    "\n",
    "NB_X_train, NB_X_test, NB_y_train, NB_y_test = train_test_split(text_tf, data['sentiment'], test_size=0.3, random_state=123)\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(NB_X_train, NB_y_train)\n",
    "predicted= clf.predict(NB_X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(NB_y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize each tweet to be an array of words\n",
    "* Double check if needed because we are already using Keras tokenizer... however, I think Keras Tokenizer is doing something differen ie: One Hot enoding based off of unique vocab.. not 100% though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.apply(lambda row: word_tokenize(row['content']), axis=1)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Class - Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "output_classes = label_encoder.fit(data['sentiment'])\n",
    "output_classes = label_encoder.transform(data['sentiment'])\n",
    "output_classes = to_categorical(output_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'enthusiasm', 'fun', 'happiness', 'hate', 'love',\n",
       "       'neutral', 'sadness', 'surprise', 'worry'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer will convert the tweets to sequences so that they can be passed through embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from two tokenizers to one. fit transofrm then just transform\n",
    "\n",
    "tk = Tokenizer(filters='')\n",
    "\n",
    "tk.fit_on_texts(X_train)\n",
    "tk.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(X_train)\n",
    "test_tokenized = tk.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "\n",
    "# labels = to_categorical(np.asarray(data['sentiment'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "**texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Glove embeddings into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "# embeddings_index = {}\n",
    "# file = open('../data/glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "# for line in file:\n",
    "#     values = line.split(' ')\n",
    "#     word = values[0] # The first entry is the word\n",
    "#     coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "#     embeddings_index[word] = coefs\n",
    "# file.close()\n",
    "\n",
    "\n",
    "glove_file = '../data/glove.twitter.27B/glove.twitter.27B.100d.txt'\n",
    "emb_dict = {}\n",
    "glove = open(glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "'''\n",
    "Create a matrix of one embedding for each word in the training dataset. \n",
    "We can do that by enumerating all unique words in the Tokenizer.word_index \n",
    "and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "'''\n",
    "\n",
    "vocab_size = len(tk.word_index) + 1\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tk.word_index.items():\n",
    "    embedding_vector = emb_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is VAL_SIZE correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = len(X_train)  # Size of the validation set\n",
    "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 100  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer without Glove weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                8010      \n",
      "=================================================================\n",
      "Total params: 88,010\n",
      "Trainable params: 88,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 2.2984 - acc: 0.1400\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 92us/step - loss: 2.2603 - acc: 0.3200\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 81us/step - loss: 2.2219 - acc: 0.3000\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 81us/step - loss: 2.1785 - acc: 0.3000\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 79us/step - loss: 2.1395 - acc: 0.3000\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 84us/step - loss: 2.0904 - acc: 0.3000\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 111us/step - loss: 2.0392 - acc: 0.3000\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 148us/step - loss: 1.9887 - acc: 0.3200\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 108us/step - loss: 1.9398 - acc: 0.3200\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 80us/step - loss: 1.8863 - acc: 0.3600\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 143us/step - loss: 1.8389 - acc: 0.4200\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 86us/step - loss: 1.7967 - acc: 0.4200\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 94us/step - loss: 1.7541 - acc: 0.4000\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 86us/step - loss: 1.7183 - acc: 0.4400\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 84us/step - loss: 1.6917 - acc: 0.4800\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 89us/step - loss: 1.6698 - acc: 0.4800\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 74us/step - loss: 1.6561 - acc: 0.5200\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 83us/step - loss: 1.6449 - acc: 0.5200\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 109us/step - loss: 1.6363 - acc: 0.5200\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 262us/step - loss: 1.6327 - acc: 0.4800\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 223us/step - loss: 1.6266 - acc: 0.4400\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 117us/step - loss: 1.6201 - acc: 0.4400\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 161us/step - loss: 1.6136 - acc: 0.5200\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 149us/step - loss: 1.6072 - acc: 0.5200\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 102us/step - loss: 1.5991 - acc: 0.5200\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 99us/step - loss: 1.5902 - acc: 0.5200\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 146us/step - loss: 1.5804 - acc: 0.5200\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 131us/step - loss: 1.5723 - acc: 0.5400\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 158us/step - loss: 1.5625 - acc: 0.5400\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 121us/step - loss: 1.5523 - acc: 0.5400\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 166us/step - loss: 1.5430 - acc: 0.5800\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 134us/step - loss: 1.5323 - acc: 0.5800\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 130us/step - loss: 1.5217 - acc: 0.6000\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 101us/step - loss: 1.5108 - acc: 0.6800\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 124us/step - loss: 1.5011 - acc: 0.7400\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 126us/step - loss: 1.4897 - acc: 0.7400\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 148us/step - loss: 1.4784 - acc: 0.8000\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 91us/step - loss: 1.4668 - acc: 0.8000\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 91us/step - loss: 1.4544 - acc: 0.8200\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 145us/step - loss: 1.4421 - acc: 0.8200\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 198us/step - loss: 1.4304 - acc: 0.8200\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 125us/step - loss: 1.4169 - acc: 0.8200\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 87us/step - loss: 1.4033 - acc: 0.8200\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 146us/step - loss: 1.3899 - acc: 0.8200\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 103us/step - loss: 1.3760 - acc: 0.8200\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 109us/step - loss: 1.3619 - acc: 0.8200\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 102us/step - loss: 1.3471 - acc: 0.8200\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 101us/step - loss: 1.3317 - acc: 0.8200\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 122us/step - loss: 1.3183 - acc: 0.8200\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 190us/step - loss: 1.3017 - acc: 0.8200\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 395us/step - loss: 1.2862 - acc: 0.8200\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 127us/step - loss: 1.2700 - acc: 0.8200\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 119us/step - loss: 1.2540 - acc: 0.8200\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 88us/step - loss: 1.2370 - acc: 0.8200\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 101us/step - loss: 1.2209 - acc: 0.8200\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 125us/step - loss: 1.2047 - acc: 0.8200\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 84us/step - loss: 1.1881 - acc: 0.8200\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 131us/step - loss: 1.1703 - acc: 0.8200\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 112us/step - loss: 1.1537 - acc: 0.8200\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 96us/step - loss: 1.1368 - acc: 0.8400\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 99us/step - loss: 1.1202 - acc: 0.8400\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 91us/step - loss: 1.1029 - acc: 0.8400\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 85us/step - loss: 1.0857 - acc: 0.8400\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 78us/step - loss: 1.0679 - acc: 0.8400\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 91us/step - loss: 1.0508 - acc: 0.8400\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 127us/step - loss: 1.0336 - acc: 0.8400\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 90us/step - loss: 1.0167 - acc: 0.8400\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 114us/step - loss: 0.9991 - acc: 0.8600\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 97us/step - loss: 0.9822 - acc: 0.8600\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 92us/step - loss: 0.9653 - acc: 0.8800\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 102us/step - loss: 0.9483 - acc: 0.8800\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 108us/step - loss: 0.9309 - acc: 0.8800\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 89us/step - loss: 0.9142 - acc: 0.8800\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 100us/step - loss: 0.8977 - acc: 0.8800\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 104us/step - loss: 0.8811 - acc: 0.8800\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 125us/step - loss: 0.8648 - acc: 0.8800\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 102us/step - loss: 0.8488 - acc: 0.8800\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 97us/step - loss: 0.8325 - acc: 0.8800\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 96us/step - loss: 0.8174 - acc: 0.8800\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 115us/step - loss: 0.8017 - acc: 0.8800\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 90us/step - loss: 0.7860 - acc: 0.8800\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 95us/step - loss: 0.7710 - acc: 0.8800\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 158us/step - loss: 0.7562 - acc: 0.8800\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 116us/step - loss: 0.7411 - acc: 0.8800\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 84us/step - loss: 0.7269 - acc: 0.9000\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 103us/step - loss: 0.7122 - acc: 0.9000\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 109us/step - loss: 0.6988 - acc: 0.9000\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 101us/step - loss: 0.6845 - acc: 0.9000\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 102us/step - loss: 0.6711 - acc: 0.9200\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 98us/step - loss: 0.6578 - acc: 0.9200\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 102us/step - loss: 0.6450 - acc: 0.9200\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 90us/step - loss: 0.6318 - acc: 0.9200\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 111us/step - loss: 0.6199 - acc: 0.9200\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 97us/step - loss: 0.6071 - acc: 0.9200\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 109us/step - loss: 0.5952 - acc: 0.9200\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 104us/step - loss: 0.5835 - acc: 0.9200\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 106us/step - loss: 0.5716 - acc: 0.9200\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 107us/step - loss: 0.5607 - acc: 0.9200\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 105us/step - loss: 0.5494 - acc: 0.9200\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 126us/step - loss: 0.5387 - acc: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e477748>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = Sequential()\n",
    "emb_model.add(Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
    "emb_model.add(Flatten())\n",
    "emb_model.add(Dense(10, activation='softmax'))\n",
    "# emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)\n",
    "# compile the model\n",
    "emb_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(emb_model.summary())\n",
    "# fit the model\n",
    "emb_model.fit(X_train, output_classes, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Glove Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                100010    \n",
      "=================================================================\n",
      "Total params: 1,100,010\n",
      "Trainable params: 1,100,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "glove_model.add(Flatten())\n",
    "glove_model.add(Dense(10, activation='softmax'))\n",
    "glove_model.summary()\n",
    "\n",
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.3201 - acc: 0.1200\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 95us/step - loss: 2.0155 - acc: 0.4800\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 97us/step - loss: 1.7750 - acc: 0.6800\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 113us/step - loss: 1.5799 - acc: 0.7600\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 108us/step - loss: 1.4121 - acc: 0.8000\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 106us/step - loss: 1.2688 - acc: 0.8800\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 103us/step - loss: 1.1532 - acc: 0.9000\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 114us/step - loss: 1.0523 - acc: 0.9000\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 177us/step - loss: 0.9658 - acc: 0.9400\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 158us/step - loss: 0.8912 - acc: 0.9400\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 155us/step - loss: 0.8249 - acc: 0.9600\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 113us/step - loss: 0.7665 - acc: 0.9600\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 114us/step - loss: 0.7142 - acc: 0.9600\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 113us/step - loss: 0.6662 - acc: 0.9600\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 100us/step - loss: 0.6252 - acc: 0.9800\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 110us/step - loss: 0.5854 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 208us/step - loss: 0.5501 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 141us/step - loss: 0.5199 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 136us/step - loss: 0.4891 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 157us/step - loss: 0.4634 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 121us/step - loss: 0.4386 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 101us/step - loss: 0.4156 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 106us/step - loss: 0.3956 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 108us/step - loss: 0.3768 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 131us/step - loss: 0.3588 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 136us/step - loss: 0.3422 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 122us/step - loss: 0.3276 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 130us/step - loss: 0.3141 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 111us/step - loss: 0.2995 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 128us/step - loss: 0.2878 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 401us/step - loss: 0.2762 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 150us/step - loss: 0.2659 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 179us/step - loss: 0.2557 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 104us/step - loss: 0.2455 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 96us/step - loss: 0.2365 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 96us/step - loss: 0.2280 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 113us/step - loss: 0.2198 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 120us/step - loss: 0.2129 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 121us/step - loss: 0.2053 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 198us/step - loss: 0.1979 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 94us/step - loss: 0.1917 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 123us/step - loss: 0.1850 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 147us/step - loss: 0.1791 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 117us/step - loss: 0.1735 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 100us/step - loss: 0.1679 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 98us/step - loss: 0.1628 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 97us/step - loss: 0.1578 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 123us/step - loss: 0.1531 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 165us/step - loss: 0.1484 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 133us/step - loss: 0.1441 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1435b2b70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.fit(X_train, output_classes, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map indexes to classes for label decoding during prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'empty',\n",
       " 1: 'enthusiasm',\n",
       " 2: 'fun',\n",
       " 3: 'happiness',\n",
       " 4: 'hate',\n",
       " 5: 'love',\n",
       " 6: 'neutral',\n",
       " 7: 'sadness',\n",
       " 8: 'surprise',\n",
       " 9: 'worry'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_class_dict = {}\n",
    "\n",
    "for index in range(0, len(label_encoder.classes_)):\n",
    "    output_class_dict[index] = label_encoder.classes_[index]\n",
    "    \n",
    "output_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10      neutral\n",
       "13      sadness\n",
       "30         hate\n",
       "46        worry\n",
       "18        worry\n",
       "0         empty\n",
       "40    happiness\n",
       "12      sadness\n",
       "29        worry\n",
       "8       sadness\n",
       "21          fun\n",
       "47      sadness\n",
       "11        worry\n",
       "41          fun\n",
       "5         worry\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = glove_model.predict(X_test) # Prediction on test data\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over probabilities of each emotion's classification\n",
    "for sentiment in y_pred:\n",
    "    max_val = np.where(sentiment == np.amax(sentiment))\n",
    "#     print(max_val[0][0])\n",
    "    results.append(output_class_dict[max_val[0][0]])\n",
    "\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is experimenting with other NLP techniques, will come back to if current methods are overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub \n",
    "import tensorflow as tf \n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"content\"], \n",
    "                                                  data['sentiment'],  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "  embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    # return average of ELMo features\n",
    "    return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train = [X_train[i:i+100] for i in range(0,X_train.shape[0],100)]\n",
    "list_test = [X_test[i:i+100] for i in range(0,X_test.shape[0],100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-b65116904d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract ELMo embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0melmo_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0melmo_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-355-b65116904d38>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract ELMo embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0melmo_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0melmo_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4372\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4373\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4374\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4375\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "# Extract ELMo embeddings \n",
    "elmo_train = [elmo_vectors(x['content']) for x in list_train] \n",
    "elmo_test = [elmo_vectors(x['content']) for x in list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save elmo_train_new\n",
    "pickle_out = open(\"elmo_train_07012019.pickle\",\"wb\")\n",
    "pickle.dump(elmo_train_new, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# save elmo_test_new\n",
    "pickle_out = open(\"elmo_test_07012019.pickle\",\"wb\")\n",
    "pickle.dump(elmo_test_new, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load elmo_train_new\n",
    "pickle_in = open(\"elmo_train_03032019.pickle\", \"rb\")\n",
    "elmo_train_new = pickle.load(pickle_in)\n",
    "\n",
    "# load elmo_train_new\n",
    "pickle_in = open(\"elmo_test_03032019.pickle\", \"rb\")\n",
    "elmo_test_new = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(elmo_train_new, \n",
    "                                                  data['content'],  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for classification with ELMO Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "model = Sequential()\n",
    "# Embedding layer using Glove Pretrained weights\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "# Flattening to conncet to Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(X_train, output_classes, epochs=50, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# lreg = LogisticRegression()\n",
    "# lreg.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to get done by Thursday\n",
    "* Feed Embedding layer to LSTM layer and stacked LSTM layer\n",
    "* Evaluate and compare the different Embedding models w/ each other and do hyper-parameter tuning\n",
    "* Visualize classification results\n",
    "* Model Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
    "\n",
    "We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 100, 100)          27100     \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 37,101\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 27,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 9 which is outside the valid range of [0, 1).  Label values: 7 6 6 6 9 7 7 9 9 2 7 9 5 6 9 7 8 9 7 3 7 9 7 9 0 9 6 2 4 1 0 6\n\t [[{{node loss_15/dense_20_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _class=[\"loc:@train...s_grad/mul\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_15/dense_20_loss/Log, loss_15/dense_20_loss/Cast)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-897bea95d1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 9 which is outside the valid range of [0, 1).  Label values: 7 6 6 6 9 7 7 9 9 2 7 9 5 6 9 7 8 9 7 3 7 9 7 9 0 9 6 2 4 1 0 6\n\t [[{{node loss_15/dense_20_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _class=[\"loc:@train...s_grad/mul\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_15/dense_20_loss/Log, loss_15/dense_20_loss/Cast)]]"
     ]
    }
   ],
   "source": [
    "# Want to add Stacked LSTM\n",
    "\n",
    "# Added Recurrent single LSTM layer, \n",
    "# model.add(LSTM(64, return_sequences=False,dropout=0.1, recurrent_dropout=0.1))\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "# Init model\n",
    "model = Sequential()\n",
    "# Embedding layer using Glove Pretrained weights\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "# Flattening to conncet to Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(X_train, output_classes, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Notes\n",
    "* Need to remove mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Medium Research Notes:\n",
    "\n",
    "If we need to create a embedding mapping for words with training, how does this affect how we embed the test data (streamed tweets)? Maybe just use Glove.\n",
    "\n",
    "Shall we use Embedding layer as our input layer?\n",
    "\n",
    "Preprocessing\n",
    "KD article\n",
    "Padding\n",
    "Remove stop words\n",
    "\n",
    "\n",
    "CNN training is 1.5 times faster than training LSTMs\n",
    "\n",
    "Need to determine the unique vocabulary of training in order to determine embedding vector length, then I think you pad off of that? Not sure about the padding preprocessing step.\n",
    "\n",
    "Pathtoword embeddings is the path where we’ve  downloaded word embeddings via glove or number batch\n",
    "\n",
    "Is it cheating if I understand all the processing of what this article is doing, and add my own preprocessing, and maybe additional steps? \n",
    "https://medium.com/@panghalarsh/sentiment-analysis-in-python-using-keras-glove-twitter-word-embeddings-and-deep-rnn-on-a-combined-580646cb900a\n",
    "\n",
    "Word Embeddings is better than count vectorizers that one hot encodes sentences because the bagofwords model will not scale well to large datasets due to create sparse vectors in high dimensionality. Also we lose semantics through BagofWords approach. Word embeddings are dense vectors with much lower dimensionality. Secondly, the semantic relationships between words are reflected in the distance and direction of the vectors.\n",
    "\n",
    "Should read into each Keras library being utilized and understanding \n",
    "\n",
    "\n",
    "Need to encode output classes (emotions) w/ LabelEncoders\n",
    "\n",
    "Why do we need to Flatten after we do embedding? Like what does that do?\n",
    "\n",
    "In the Embedding layer (which is layer 0 here) we set the weights for the words to those found in the GloVe word embeddings. By setting trainable to False we make sure that the GloVe word embeddings cannot be changed.\n",
    "\n",
    "The best result is achieved with 100-dimensional word embeddings that are trained on the available data. By doing this, we do not take into account the relationships between the words in the tweet. This can be achieved with a recurrent neural network or a 1D convolutional network. But that’s something for a future post.\n",
    "— Bert Carremans \n",
    "\n",
    "\n",
    "Do we need padding when utilizing word embeddings? I saw some Word2Vec tutorials using padding one hot encoded sequences, but my understanding was that we don’t need it for embeddings.\n",
    "https://medium.com/datadriveninvestor/sentiment-analysis-using-embeddings-f3dd99aeaade\n",
    "\n",
    "\n",
    "Why train my own word embeddings at all? Why not always use Glove? It seems to be a better trained for my model’s embedding layer anyways.\n",
    "\n",
    "\n",
    "There are so many state of the art embedding techniques like SkipGrams, FastText, and ELMo. I should investigate these for their result to text summarizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 1 2 3 6]]\n",
      "[[[ 0.01658997 -0.03503971 -0.03524492]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [ 0.02627129 -0.02832104  0.01373578]]\n",
      "\n",
      " [[-0.03747336 -0.02579566  0.03765706]\n",
      "  [ 0.02105062 -0.02567688 -0.02039844]\n",
      "  [-0.02736325 -0.00165455  0.00108389]\n",
      "  [-0.01605218  0.04671435 -0.04131665]\n",
      "  [-0.04258297  0.01286627  0.01142017]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "model = Sequential()\n",
    "model.add(Embedding(7, 3, input_length=5))\n",
    "# model.add(Flatten())\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.array([[0, 1, 2, 3, 4], [5, 1, 2, 3, 6]])\n",
    "print(input_array)\n",
    "# model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ELMo's pretrained model\n",
    "* When we import this model, should keep it trainable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
